#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import feedparser
import os
import sys
from datetime import datetime, timezone, timedelta
import xml.etree.ElementTree as ET
from xml.dom import minidom
import json
from urllib.parse import urlparse, urlunparse
from email.utils import parsedate_to_datetime

# -----------------------------
# CONFIGURATION
# -----------------------------
FEEDS = [
    "https://politepol.com/fd/lRzLqNhRg2jV.xml",
    "https://politepol.com/fd/LWVzWA8NSHfJ.xml",
    "https://evilgodfahim.github.io/juop/editorial_news.xml",
]

MASTER_FILE = "feed_master.xml"
DAILY_FILE = "daily_feed.xml"
DAILY_FILE_2 = "daily_feed_2.xml"
LAST_SEEN_FILE = "last_seen.json"

MAX_ITEMS = 1000
BD_OFFSET = 6
LOOKBACK_HOURS = 48
LINK_RETENTION_DAYS = 7

# -----------------------------
# LINK NORMALIZER
# -----------------------------
def normalize_link(url):
    if not url:
        return ""
    parsed = urlparse(url)
    path = parsed.path.rstrip("/")
    normalized = urlunparse((parsed.scheme, parsed.netloc, path, parsed.params, parsed.query, parsed.fragment))
    return normalized

# -----------------------------
# SOURCE EXTRACTOR
# -----------------------------
def extract_source(link):
    try:
        host = urlparse(link).netloc.lower()
        host = host.replace("www.", "")
        parts = host.split(".")
        return parts[0] if len(parts) >= 2 else host
    except:
        return "unknown"

# -----------------------------
# DATE PARSER
# -----------------------------
def parse_date(entry):
    # Try feedparser parsed tuples
    for f in ("published_parsed", "updated_parsed", "created_parsed"):
        t = entry.get(f) if isinstance(entry, dict) else getattr(entry, f, None)
        if t:
            try:
                return datetime(*t[:6], tzinfo=timezone.utc)
            except:
                continue

    # Try string fields
    for key in ("published", "updated", "pubDate", "created"):
        val = entry.get(key) if isinstance(entry, dict) else getattr(entry, key, None)
        if val:
            try:
                dt = parsedate_to_datetime(val)
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                return dt.astimezone(timezone.utc)
            except:
                continue

    # fallback
    return datetime.now(timezone.utc)

# -----------------------------
# LOAD EXISTING FEED
# -----------------------------
def load_existing(file_path):
    if not os.path.exists(file_path):
        return []
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
    except:
        return []

    items = []
    for item in root.findall(".//item"):
        try:
            title = (item.find("title").text or "").strip()
            link = normalize_link(item.find("link").text) if item.find("link") is not None else ""
            desc = item.find("description").text or ""
            pub_node = item.find("pubDate")
            pubDate = datetime.now(timezone.utc)
            if pub_node is not None and pub_node.text:
                try:
                    pubDate = parsedate_to_datetime(pub_node.text)
                    if pubDate.tzinfo is None:
                        pubDate = pubDate.replace(tzinfo=timezone.utc)
                    pubDate = pubDate.astimezone(timezone.utc)
                except:
                    pubDate = datetime.now(timezone.utc)
            items.append({"title": title, "link": link, "description": desc, "pubDate": pubDate})
        except:
            continue
    return items

# -----------------------------
# WRITE RSS
# -----------------------------
def write_rss(items, file_path, title="Feed"):
    # Ensure unique pubDate by microsecond increment
    seen_times = set()
    for i, item in enumerate(items):
        while item["pubDate"] in seen_times:
            item["pubDate"] += timedelta(microseconds=1)
        seen_times.add(item["pubDate"])

    rss = ET.Element("rss", version="2.0")
    channel = ET.SubElement(rss, "channel")
    ET.SubElement(channel, "title").text = title
    ET.SubElement(channel, "link").text = "https://evilgodfahim.github.io/"
    ET.SubElement(channel, "description").text = f"{title} generated by script"

    for item in items:
        it = ET.SubElement(channel, "item")
        ET.SubElement(it, "title").text = item.get("title", "")
        ET.SubElement(it, "link").text = item.get("link", "")
        ET.SubElement(it, "description").text = item.get("description", "")
        pub = item.get("pubDate")
        ET.SubElement(it, "pubDate").text = pub.strftime("%a, %d %b %Y %H:%M:%S %z") if isinstance(pub, datetime) else str(pub)

    xml_str = minidom.parseString(ET.tostring(rss)).toprettyxml(indent="  ")
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(xml_str)

# -----------------------------
# LAST SEEN TRACKING
# -----------------------------
def load_last_seen():
    if os.path.exists(LAST_SEEN_FILE):
        try:
            with open(LAST_SEEN_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                last_seen_str = data.get("last_seen")
                processed = set(data.get("processed_links", []))
                last_seen_dt = datetime.fromisoformat(last_seen_str) if last_seen_str else None
                return {"last_seen": last_seen_dt, "processed_links": processed}
        except:
            return {"last_seen": None, "processed_links": set()}
    return {"last_seen": None, "processed_links": set()}

def save_last_seen(last_dt, processed_links, master_items):
    cutoff = last_dt - timedelta(days=LINK_RETENTION_DAYS)
    master_links_recent = {item["link"] for item in master_items if item["pubDate"] > cutoff}
    links_to_keep = [link for link in processed_links if link in master_links_recent]
    with open(LAST_SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump({"last_seen": last_dt.isoformat(), "processed_links": links_to_keep}, f, indent=2)

# -----------------------------
# MASTER FEED UPDATE
# -----------------------------
def update_master():
    print("[Updating master feed]")
    existing = load_existing(MASTER_FILE)
    existing_links = {x["link"] for x in existing}
    existing_titles = {x["title"] for x in existing}
    new_items = []

    for url in FEEDS:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                link = normalize_link(entry.get("link", ""))
                title_raw = entry.get("title", "")
                title = title_raw.strip()
                source = extract_source(link)
                final_title = f"{title}. [ {source} ]" if title else f"No Title. [ {source} ]"
                if link in existing_links or final_title in existing_titles:
                    continue

                desc = entry.get("summary", "") or ""
                if not desc:
                    content = entry.get("content")
                    if content:
                        if isinstance(content, list) and isinstance(content[0], dict):
                            desc = content[0].get("value", "")
                        elif isinstance(content, str):
                            desc = content

                pub_dt = parse_date(entry)

                new_items.append({"title": final_title, "link": link, "description": desc, "pubDate": pub_dt})
                existing_links.add(link)
                existing_titles.add(final_title)
        except Exception as e:
            print(f"Error parsing {url}: {e}")

    all_items = existing + new_items
    all_items.sort(key=lambda x: x["pubDate"], reverse=True)
    all_items = all_items[:MAX_ITEMS]

    if not all_items:
        all_items = [{"title": "No articles yet", "link": "https://evilgodfahim.github.io/", "description": "Master feed will populate after first successful fetch.", "pubDate": datetime.now(timezone.utc)}]

    write_rss(all_items, MASTER_FILE, title="Master Feed (Updated every 30 mins)")
    print(f"âœ“ Master feed updated with {len(all_items)} items ({len(new_items)} new)")

# -----------------------------
# DAILY FEED UPDATE
# -----------------------------
def update_daily():
    print("[Updating daily feed]")
    to_zone = timezone(timedelta(hours=BD_OFFSET))
    last_data = load_last_seen()
    last_seen_dt = last_data["last_seen"]
    processed_links = set(last_data["processed_links"])
    lookback_dt = (last_seen_dt - timedelta(hours=LOOKBACK_HOURS)) if last_seen_dt else None

    master_items = load_existing(MASTER_FILE)
    new_items = []

    for item in master_items:
        link = item["link"]
        pub = item["pubDate"].astimezone(to_zone)
        if link in processed_links:
            continue
        if not lookback_dt or pub > lookback_dt:
            new_items.append(item)
            processed_links.add(link)

    if not new_items:
        placeholder = [{"title": "No new articles today", "link": "https://evilgodfahim.github.io/", "description": "Daily feed will populate after first articles appear.", "pubDate": datetime.now(timezone.utc)}]
        write_rss(placeholder, DAILY_FILE, title="Daily Feed (Updated 9 AM BD)")
        write_rss([], DAILY_FILE_2, title="Daily Feed Extra (Updated 9 AM BD)")
        save_last_seen(placeholder[0]["pubDate"], processed_links, master_items)
        return

    new_items.sort(key=lambda x: x["pubDate"], reverse=True)
    write_rss(new_items[:100], DAILY_FILE, title="Daily Feed (Updated 9 AM BD)")
    write_rss(new_items[100:], DAILY_FILE_2, title="Daily Feed Extra (Updated 9 AM BD)")
    save_last_seen(max(i["pubDate"] for i in new_items), processed_links, master_items)

# -----------------------------
# MAIN
# -----------------------------
if __name__ == "__main__":
    args = sys.argv[1:]
    if "--master-only" in args:
        update_master()
    elif "--daily-only" in args:
        update_daily()
    else:
        update_master()
        update_daily()