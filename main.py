import feedparser
import os
import sys
from datetime import datetime, timezone, timedelta
import xml.etree.ElementTree as ET
from xml.dom import minidom
import json
import re
from urllib.parse import urlparse, urlunparse

# -----------------------------
# CONFIGURATION
# -----------------------------
FEEDS = [
    "https://politepol.com/fd/lRzLqNhRg2jV.xml",
    "https://politepol.com/fd/LWVzWA8NSHfJ.xml",
    "https://evilgodfahim.github.io/juop/editorial_news.xml",
    "https://evilgodfahim.github.io/bbop/feed.xml",
    "https://evilgodfahim.github.io/bdpratidin-rss/feed.xml",
    "https://fetchrss.com/feed/aLNkZSZkMOtSaLNkNF2oqA-i.rss",
    "https://politepol.com/fd/4LWXWOY5wPR9.xml",
    "https://politepol.com/fd/VnoJt9i4mZPJ.xml",
    "https://evilgodfahim.github.io/sop/opinion_feed.xml",
    "https://politepol.com/fd/tqu8P8uIlNm1.xml",
    "https://feeds.bbci.co.uk/bengali/rss.xml",
    "https://politepol.com/fd/YgbESpqhLwdK.xml",
    "https://politepol.com/fd/TnjwLaSLd1M8.xml",
    "https://politepol.com/fd/e0zKTeKoRpXa.xml",
    "https://evilgodfahim.github.io/kk/opinion.xml",
    "https://politepol.com/fd/1yC3YJpL3i6t.xml",
    "https://politepol.com/fd/aPXIv1Q7cs7S.xml",
    "https://politepol.com/fd/eYS0c238EjkY.xml",
    "https://evilgodfahim.github.io/banglanews/opinion.xml",
    "https://evilgodfahim.github.io/kalbela/opinion.xml",
    "https://evilgodfahim.github.io/samakal/feeds/opinion.xml",
    "https://politepol.com/fd/dwg0cNjfFTLe.xml",
    "https://politepol.com/fd/RW7B9eQ8SuQ8.xml",
    "https://politepol.com/fd/Om635UbkdlGQ.xml",
    "https://politepol.com/fd/iBikrmLHw51t.xml",
    "https://politepol.com/fd/joNpOlIQpxws.xml",
    "https://politepol.com/fd/xwWyLagKzYe1.xml",
    "https://evilgodfahim.github.io/juop/tp_editorial_news.xml",
    "https://politepol.com/fd/OM5MULjADosd.xml",
"https://politepol.com/fd/FvaPzwOZSVaI.xml",
"https://politepol.com/fd/CxsnfXBZ1EMn.xml"
]

MASTER_FILE = "feed_master.xml"
DAILY_FILE = "daily_feed.xml"
LAST_SEEN_FILE = "last_seen.json"

MAX_ITEMS = 1000
BD_OFFSET = 6

# -----------------------------
# LINK NORMALIZER
# -----------------------------
def normalize_link(url):
    if not url:
        return ""
    parsed = urlparse(url)
    path = parsed.path.rstrip("/")

    m = re.match(r"^(.*?/op-ed/[^/]+)(/op-ed/[^/]+)$", path)
    if m:
        path = m.group(1)

    normalized = urlunparse((parsed.scheme, parsed.netloc, path, "", "", ""))
    return normalized

# -----------------------------
# UTILITIES
# -----------------------------
def parse_date(entry):
    fields = ["published_parsed", "updated_parsed", "created_parsed"]
    for f in fields:
        t = getattr(entry, f, None)
        if t:
            return datetime(*t[:6], tzinfo=timezone.utc)
    return datetime.now(timezone.utc)

def load_existing(file_path):
    if not os.path.exists(file_path):
        return []
    tree = ET.parse(file_path)
    root = tree.getroot()
    items = []
    for item in root.findall(".//item"):
        try:
            title = item.find("title").text or ""
            link = normalize_link(item.find("link").text or "")
            desc = item.find("description").text or ""
            pubDate = item.find("pubDate").text or ""
            dt = datetime.strptime(pubDate, "%a, %d %b %Y %H:%M:%S %z")
            items.append({"title": title, "link": link, "description": desc, "pubDate": dt})
        except:
            continue
    return items

def write_rss(items, file_path, title="Feed"):
    rss = ET.Element("rss", version="2.0")
    channel = ET.SubElement(rss, "channel")
    ET.SubElement(channel, "title").text = title
    ET.SubElement(channel, "link").text = "https://evilgodfahim.github.io/"
    ET.SubElement(channel, "description").text = f"{title} generated by script"

    for item in items:
        it = ET.SubElement(channel, "item")
        ET.SubElement(it, "title").text = item["title"]
        ET.SubElement(it, "link").text = item["link"]
        ET.SubElement(it, "description").text = item["description"]
        ET.SubElement(it, "pubDate").text = item["pubDate"].strftime("%a, %d %b %Y %H:%M:%S %z")

    xml_str = minidom.parseString(ET.tostring(rss)).toprettyxml(indent="  ")
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(xml_str)

# -----------------------------
# MASTER FEED UPDATE
# -----------------------------
def update_master():
    print("[Updating feed_master.xml]")

    existing = load_existing(MASTER_FILE)

    existing_links = {x["link"] for x in existing}
    existing_titles = {x["title"].strip() for x in existing}

    new_items = []

    for url in FEEDS:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                raw_link = getattr(entry, "link", "")
                link = normalize_link(raw_link)
                title = getattr(entry, "title", "").strip()

                # --- NEW RULE: skip if EITHER link OR title exists ---
                if link in existing_links or title in existing_titles:
                    continue

                new_items.append({
                    "title": title if title else "No Title",
                    "link": link,
                    "description": getattr(entry, "summary", ""),
                    "pubDate": parse_date(entry)
                })

                existing_links.add(link)
                existing_titles.add(title)

        except Exception as e:
            print(f"Error parsing {url}: {e}")

    all_items = existing + new_items
    all_items.sort(key=lambda x: x["pubDate"], reverse=True)
    all_items = all_items[:MAX_ITEMS]

    if not all_items:
        all_items = [{
            "title": "No articles yet",
            "link": "https://evilgodfahim.github.io/",
            "description": "Master feed will populate after first successful fetch.",
            "pubDate": datetime.now(timezone.utc)
        }]

    write_rss(all_items, MASTER_FILE, title="Master Feed (Updated every 30 mins)")
    print(f"✓ feed_master.xml updated with {len(all_items)} items")

# -----------------------------
# DAILY FEED UPDATE
# -----------------------------
def update_daily():
    print("[Updating daily_feed.xml]")
    to_zone = timezone(timedelta(hours=BD_OFFSET))

    if os.path.exists(LAST_SEEN_FILE):
        with open(LAST_SEEN_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            last = data.get("last_seen")
            last_seen_dt = datetime.fromisoformat(last) if last else None
    else:
        last_seen_dt = None

    master_items = load_existing(MASTER_FILE)
    new_items = []

    for item in master_items:
        pub = item["pubDate"].astimezone(to_zone)
        if not last_seen_dt or pub > last_seen_dt:
            new_items.append(item)

    if not new_items:
        new_items = [{
            "title": "No new articles today",
            "link": "https://evilgodfahim.github.io/",
            "description": "Daily feed will populate after first articles appear.",
            "pubDate": datetime.now(timezone.utc)
        }]

    write_rss(new_items, DAILY_FILE, title="Daily Feed (Updated 9 AM BD)")

    last_dt = max([i["pubDate"] for i in new_items])
    with open(LAST_SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump({"last_seen": last_dt.isoformat()}, f)

    print(f"✓ daily_feed.xml updated with {len(new_items)} items")

# -----------------------------
# MAIN
# -----------------------------
if __name__ == "__main__":
    args = sys.argv[1:]
    if "--master-only" in args:
        update_master()
    elif "--daily-only" in args:
        update_daily()
    else:
        update_master()
        update_daily()